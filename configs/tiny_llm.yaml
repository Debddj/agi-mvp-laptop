model:
  vocab_size: 2000
  n_layers: 6
  n_heads: 6
  hidden_size: 384
  mlp_ratio: 4
  max_seq_len: 2048

training:
  batch_size: 2
  lr: 3e-4
  epochs: 2
  grad_clip: 1.0
